{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict tags on StackOverflow with linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we predict tags for posts from [StackOverflow](https://stackoverflow.com). To solve this task you will use multilabel classification approach.\n",
    "\n",
    "The libraries used are: numpy, pandas, sklearn, nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "from ast import literal_eval\n",
    "\n",
    "import nltk\n",
    "# Required to remove english stop words from the corpus\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy import sparse as sp_sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the train/val/test dataset\n",
    "# literal_eval is a function to convert a data type in string form to it's original type.\n",
    "# The labels train['tags'] & test['tags'] need to be processed by literal_eval\n",
    "\n",
    "train = pd.read_csv('data/train.tsv', sep = '\\t')\n",
    "train['tags'] = train['tags'].apply(literal_eval)\n",
    "\n",
    "validation = pd.read_csv('data/validation.tsv', sep = '\\t')\n",
    "validation['tags'] = validation['tags'].apply(literal_eval)\n",
    "    \n",
    "test = pd.read_csv('data/test.tsv', sep = '\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How to draw a stacked dotplot in R?</td>\n",
       "      <td>[r]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mysql select all records where a datetime fiel...</td>\n",
       "      <td>[php, mysql]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to terminate windows phone 8.1 app</td>\n",
       "      <td>[c#]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>get current time in a specific country via jquery</td>\n",
       "      <td>[javascript, jquery]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Configuring Tomcat to Use SSL</td>\n",
       "      <td>[java]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title                  tags\n",
       "0                How to draw a stacked dotplot in R?                   [r]\n",
       "1  mysql select all records where a datetime fiel...          [php, mysql]\n",
       "2             How to terminate windows phone 8.1 app                  [c#]\n",
       "3  get current time in a specific country via jquery  [javascript, jquery]\n",
       "4                      Configuring Tomcat to Use SSL                [java]"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the first 5 rows of the loaded train dataset \n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning to X (features), y (labels) \n",
    "# .values gives the output in array form. Equivalent to  DataFrame.to_numpy()\n",
    "X_train, y_train = train['title'].values, train['tags'].values\n",
    "X_val, y_val = validation['title'].values, validation['tags'].values\n",
    "X_test = test['title'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text processing...\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def text_prepare(text):\n",
    "    text = text.lower() # lowercase text\n",
    "    text = re.sub(REPLACE_BY_SPACE_RE, \" \", text)   # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = re.sub(BAD_SYMBOLS_RE, \"\", text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    \n",
    "    # Removing englisg stopwords from the corpus\n",
    "    split_text = text.split()\n",
    "    text_t1 = ' '.join([word for word in split_text if word not in STOPWORDS])\n",
    "\n",
    "    return text_t1\n",
    "\n",
    "X_train = [text_prepare(x) for x in X_train]\n",
    "X_val = [text_prepare(x) for x in X_val]\n",
    "X_test = [text_prepare(x) for x in X_test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall consider 2 approaches for the training. In first approach we train with Bag of words on 5000 frequent words from the corpus. In the second approach we train with sklearn's TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing train data for the BOW (first approach)\n",
    "tags_counts = {} # Dictionary of all tags from train corpus with their counts.\n",
    "words_counts = {} # Dictionary of all words from train corpus with their counts.\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    for j in range(len(y_train[i])):\n",
    "        tag = y_train[i][j]\n",
    "        if tag in tags_counts.keys():\n",
    "            tags_counts[tag] += 1\n",
    "        else:\n",
    "            tags_counts[tag] = 1\n",
    "            \n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    word_list = X_train[i].split()\n",
    "    for j in range(len(word_list)):\n",
    "        word = word_list[j]\n",
    "        if word in words_counts.keys():\n",
    "            words_counts[word] += 1\n",
    "        else:\n",
    "            words_counts[word] = 1    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape  (100000, 5000)\n"
     ]
    }
   ],
   "source": [
    "DICT_SIZE = 5000\n",
    "\n",
    "W_COUNTS = sorted(words_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "WORDS_TO_INDEX = {q[0]: p for p,q in enumerate(W_COUNTS[:DICT_SIZE])}\n",
    "INDEX_TO_WORDS = {p: q[0] for p,q in enumerate(W_COUNTS[:DICT_SIZE])}\n",
    "ALL_WORDS = [p for p,q in w_counts[:DICT_SIZE]]\n",
    "\n",
    "# The below function takes text, size of dictionary and return a vector which is a bag-of-words representation of 'text'\n",
    "def my_bag_of_words(text, words_to_index, dict_size):    \n",
    "    result_vector = np.zeros(dict_size)\n",
    "    \n",
    "    text_list = text.split()\n",
    "    words = words_to_index.keys()\n",
    "    \n",
    "    for w in text_list:\n",
    "        if w in words:\n",
    "            ind = words_to_index[w]\n",
    "            result_vector[ind] += 1\n",
    "    \n",
    "    return result_vector\n",
    "\n",
    "# We transform the data to sparse representation, to store the information efficiently. \n",
    "X_train_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_train])\n",
    "X_val_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_val])\n",
    "X_test_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_test])\n",
    "\n",
    "print('X_train shape ', X_train_mybag.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we do our second approach i.e. TfidfVectorizer\n",
    "# Input X_train, X_val, X_test â€” samples        \n",
    "# Output TF-IDF vectorized representation of each sample and vocabulary\n",
    "    \n",
    "def tfidf_features(X_train, X_val, X_test):\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df = 5, max_df = 0.9, token_pattern = '(\\S+)')\n",
    "   \n",
    "    X_train = tfidf_vectorizer.fit_transform(X_train)   \n",
    "    X_val = tfidf_vectorizer.transform(X_val)\n",
    "    X_test = tfidf_vectorizer.transform(X_test)\n",
    "    \n",
    "    return X_train, X_val, X_test, tfidf_vectorizer.vocabulary_\n",
    "\n",
    "X_train_tfidf, X_val_tfidf, X_test_tfidf, tfidf_vocab = tfidf_features(X_train, X_val, X_test)\n",
    "tfidf_reversed_vocab = {i:word for word,i in tfidf_vocab.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a multi label classification problem. So we need to binarize the labels.\n",
    "mlb = MultiLabelBinarizer(classes=sorted(tags_counts.keys()))\n",
    "y_train = mlb.fit_transform(y_train)\n",
    "y_val = mlb.fit_transform(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the classifier. Input - training data. Output - trained classifier\n",
    "\n",
    "def train_classifier(X_train, y_train): \n",
    "    # LogisticRegression wrapped into OneVsRestClassifier\n",
    "    lr = LogisticRegression(penalty = \"l1\", solver = 'saga')\n",
    "    ovr_classifier = OneVsRestClassifier(lr)\n",
    "    ovr_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    return ovr_classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\student\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Training the classifiers for different data transformations: bag-of-words and tf-idf.\n",
    "classifier_mybag = train_classifier(X_train_mybag, y_train)\n",
    "classifier_tfidf = train_classifier(X_train_tfidf, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for the data.Two types of predictions are done here: labels and scores\n",
    "y_val_predicted_labels_mybag = classifier_mybag.predict(X_val_mybag)\n",
    "y_val_predicted_scores_mybag = classifier_mybag.decision_function(X_val_mybag)\n",
    "\n",
    "y_val_predicted_labels_tfidf = classifier_tfidf.predict(X_val_tfidf)\n",
    "y_val_predicted_scores_tfidf = classifier_tfidf.decision_function(X_val_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation 1.) Accuracy 2.) F1-score 3.) Precision score\n",
    "def print_evaluation_scores(y_val, predicted):\n",
    "    \n",
    "    print(\"Accuracy score: \", accuracy_score(y_val, predicted))\n",
    "    print(\"F-1 score weighted: \", f1_score(y_val, predicted, average='weighted'))\n",
    "    print(\"Average precision score: \", average_precision_score(y_val, predicted))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the results of prediction by BOW and TFIDF\n",
    "print('Bag-of-words')\n",
    "print_evaluation_scores(y_val, y_val_predicted_labels_mybag)\n",
    "\n",
    "print('')\n",
    "print('Tfidf')\n",
    "print_evaluation_scores(y_val, y_val_predicted_labels_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for the test dataset\n",
    "test_predictions = classifier_tfidf.predict(X_test_tfidf)\n",
    "test_pred_inversed = mlb.inverse_transform(test_predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
